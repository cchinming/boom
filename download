# -*- coding: utf-8 -*-  
import requests  
from bs4 import BeautifulSoup  
import csv  
from multiprocessing import Queue  
import  threading  
import random  
from time import sleep  
from UserAgent import HEADERS
times=0
csvfile=open('id-level.csv','w',encoding='utf-8',newline='')
writer=csv.writer(csvfile)
writer.writerow(["区域","名称","景点id","类型","级别","热度","经纬度"]) 
##print(HEADERS,User_Agent)
def download_soup_waitting(url):  
    try:  
        response= requests.get(url,headers=HEADERS,allow_redirects=False,timeout=5)  
        if response.status_code==200:  
            html=response.content  
            html=html.decode("utf-8")  
            soup = BeautifulSoup(html, "html.parser")  
            return soup  
        else:  
            sleep(random.randint(1,3))
			#max_count+=1  
            print("等待ing")
            """max_count+=1
            if max_count==100:
                print('error')
                exit()"""			
            return download_soup_waitting(url)  
    except Exception as e:  
        return e  



def getType(type,url,times=times):  
    soup=download_soup_waitting(url)
#	print(soup[0:100])
    search_list=soup.find('div', attrs={'id': 'search-list'})  
    sight_items=search_list.findAll('div', attrs={'class': 'sight_item'})  
    for sight_item in sight_items:  
        name=sight_item['data-sight-name']  
        districts=sight_item['data-districts']  
        point=sight_item['data-point']  
        #address=sight_item['data-address']  
        data_id=sight_item['data-id']  
        level=sight_item.find('span',attrs={'class':'level'})  
        if level:  
            level=level.text  
        else:  
            level=""  
        product_star_level=sight_item.find('span',attrs={'class':'product_star_level'})  
        if product_star_level:  
            product_star_level=product_star_level.text  
        else:  
            product_star_level=""  
        intro=sight_item.find('div',attrs={'class':'intro'})  
        
        writer.writerow([districts.replace("\n",""),name.replace("\n",""),data_id.replace("\n",""),type.replace("\n",""),level.replace("\n",""),product_star_level.replace("\n","")[0:5],point.replace("\n","")])  
    next=soup.find('a',attrs={'class':'next'})  
	
    if times>1000:
        print('exit')
        exit()
    if next:  
        next_url="http://piao.qunar.com"+next['href']  
        getType(type,next_url,times+1)  

def getTypes():  
    types=["文化古迹","自然风光","公园","古建筑","寺庙","遗址","古镇","陵墓陵园","故居","宗教"] #实际不止这些分组 需要自己补充  
    for type in types:  
        url="http://piao.qunar.com/ticket/list.htm?keyword=%E7%83%AD%E9%97%A8%E6%99%AF%E7%82%B9&region=&from=mpl_search_suggest&subject="+type+"&page=1"  
        getType(type,url) 

if __name__ == '__main__':  
    getTypes()




